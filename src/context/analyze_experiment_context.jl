abstract type AbstractExperimentAnalysisContext <: AbstractContext end
abstract type AbstractErrorAnalysis  end
struct ReconstructionErrorAnalysis <: AbstractErrorAnalysis end

struct ReprojectionErrorAnalysis{T <: Type{HomographyMatrix}}  <: AbstractErrorAnalysis
end

struct ParameterErrorAnalysis{T <: ProjectiveEntity} <: AbstractErrorAnalysis
    projective_entity::T
end

struct ExperimentAnalysisContext{Tâ‚ <: AbstractSyntheticScene, Tâ‚‚ <: AbstractErrorAnalysis,} <: AbstractExperimentAnalysisContext
    synthetic_scene::Tâ‚
    analysis::Tâ‚‚
end


function (context::ExperimentAnalysisContext)(estimates::AbstractVector, runtimes::AbstractVector)
    synthetic_scene = context.synthetic_scene
    analyze = context.analysis

    world = get_world(synthetic_scene)
    cameras = get_cameras(synthetic_scene)
    cameraâ‚ = cameras[1]
    cameraâ‚‚ = cameras[2]

    if typeof(analyze) <: ReconstructionErrorAnalysis
        transformed_cameraâ‚  = deepcopy(cameraâ‚)
        transformed_cameraâ‚‚  = deepcopy(cameraâ‚‚)
        transformed_world = deepcopy(world)

        # Transform the coordinate system so that the first camera represents the origin of the world coordinate system.
        # This is required because the algorithm for extracting pose from a single homography matrix assumes that
        # the first camera represents origin of the world coordinate system.
        default_world_system = CartesianSystem(Point(0.0, 0.0, 0.0), Vec(1.0, 0.0, 0.0), Vec(0.0, 1.0, 0.0), Vec(0.0, 0.0, 1.0))
        alternative_world_system = get_coordinate_system(get_extrinsics(get_model(cameraâ‚)))
        transformation_context! = WorldCoordinateTransformationContext(CoordinateTransformation(source = default_world_system, target = alternative_world_system))
        transformation_context!(transformed_cameraâ‚)
        transformation_context!(transformed_cameraâ‚‚)
        transformation_context!(transformed_world)
        R = length(estimates)
        return [analyze(transformed_cameraâ‚, transformed_cameraâ‚‚, transformed_world, estimates[r]) for r = 1:R]
    else
        R = length(estimates)
        return [analyze(cameraâ‚, cameraâ‚‚, world, estimates[r]) for r = 1:R]
    end
end



function (::ReconstructionErrorAnalysis)(cameraâ‚::AbstractCamera, cameraâ‚‚::AbstractCamera, world::AbstractWorld, estimates::AbstractVector)
    analogue_imageâ‚ = get_image_type(cameraâ‚)
    analogue_imageâ‚‚ = get_image_type(cameraâ‚‚)

    # Project 3D points onto the cameras.
    aquire = AquireImageContext()
    â„³ = aquire(world, cameraâ‚)
    â„³â€² = aquire(world, cameraâ‚‚)

    extract_pose = PoseFromSingleHomographyContext(intrinsics = get_intrinsics(get_model(cameraâ‚)), image_type = analogue_imageâ‚, algorithm = MalisVargasDecomposition())
    rms_errors = zeros(Float64, length(estimates))
    for (i, estimate) in enumerate(estimates)
        ð‡áµ¢ = estimate
        allotment = get_interval(world.groups[i])
        ð’ž = Correspondences((â„³[allotment],â„³â€²[allotment]))



        potential_poses = extract_pose(HomographyMatrix(ð‡áµ¢), ð’ž)

        relative_pose = RelativePose(cameraâ‚, cameraâ‚‚)
        R = ComputerVision.rotation(relative_pose)
        t = ComputerVision.translation(relative_pose)
        planes = get_planes(world)
        plane = planes[i]
        ð§ = get_normal(plane)
        d = get_distance(plane)
        # Convention used by Malis and Vargas in their homography decomposition paper.
        ð‘ = R'
        ð­ = (-R'*t) / d

        Aâ‚, bâ‚, vâ‚ =  potential_poses[1]
        Aâ‚‚, bâ‚‚, vâ‚‚ =  potential_poses[2]

        cameraâ‚ = deepcopy(cameraâ‚)
        cameraáµ¦ = deepcopy(cameraâ‚)
        relocate!(cameraáµ¦, Aâ‚', -Aâ‚'*(bâ‚*d))
        cameraáµ§ = deepcopy(cameraâ‚)
        relocate!(cameraáµ§, Aâ‚‚', -Aâ‚‚'*(bâ‚‚*d))

        triangulate_points =  TriangulateContext(DirectLinearTriangulation())
        #estimated_points = triangulate_points(cameraâ‚, cameraâ‚‚, ð’ž)
        estimated_pointsâ‚ = triangulate_points(cameraâ‚, cameraáµ¦, ð’ž)
        estimated_pointsâ‚‚ = triangulate_points(cameraâ‚, cameraáµ§, ð’ž)
        world_points = get_points(world)
        reference_points = world_points[allotment]

        N = length(ð’ž)
        squared_errorâ‚ = compute_squared_error(estimated_pointsâ‚, reference_points)
        squared_errorâ‚‚ = compute_squared_error(estimated_pointsâ‚‚, reference_points)
        Î¼â‚ = mean(squared_errorâ‚)
        Î¼â‚‚ = mean(squared_errorâ‚‚)
        rmsâ‚ = sqrt(sum((1/(3*N)) .* squared_errorâ‚))
        rmsâ‚‚ = sqrt(sum((1/(3*N)) .* squared_errorâ‚‚))
        rms = min(rmsâ‚, rmsâ‚‚)
        rms_errors[i] = rms
    end
    rms_errors
end

function compute_squared_error(estimated_points::AbstractVector, reference_points::AbstractVector)
    [norm(first(couple)-last(couple))^2 for couple in zip(estimated_points, reference_points)]
end

function (::ReprojectionErrorAnalysis{<:Type{HomographyMatrix}})(cameraâ‚::AbstractCamera, cameraâ‚‚::AbstractCamera, world::AbstractWorld, estimates::AbstractVector)
    analogue_imageâ‚ = get_image_type(cameraâ‚)
    analogue_imageâ‚‚ = get_image_type(cameraâ‚‚)

    # Project 3D points onto the cameras.
    aquire = AquireImageContext()
    â„³ = aquire(world, cameraâ‚)
    â„³â€² = aquire(world, cameraâ‚‚)
    rms_errors = zeros(Float64, length(estimates))
    for (i, estimate) in enumerate(estimates)
        ð‡áµ¢ = estimate
        # Extract the corresponding points associated with the current planar structure.
        allotment = get_interval(world.groups[i])
        ð’ª = â„³[allotment]
        ð’ªâ€² = â„³â€²[allotment]
        N = length(ð’ª)

        # Construct a length-(2*N) vector consisting of N two-dimensional points in the
        # first view.
        ð›‰ = Vector{Float64}(undef, N*2)
        k = 1
        for n = 1:N
            ð›‰[k:k+1] = @view ð’ª[n][1:2]
            k = k + 2
        end

        indexâ‚ = SVector(1,2)
        indexâ‚‚ = SVector(3,4)
        pts = Matrix{Float64}(undef,4,N)

        for n = 1:N
             pts[indexâ‚,n] = ð’ª[n][indexâ‚]
             pts[indexâ‚‚,n] = ð’ªâ€²[n][indexâ‚]
        end

        #fit = curve_fit(model_homography,  ð‡, reshape(reinterpret(Float64,vec(pts)),(4*N,)) , ð›‰; show_trace = false, maxIter = 2)
        fit = curve_fit(model_homography, jacobian_model_homography, ð‡áµ¢, reshape(reinterpret(Float64,vec(pts)),(4*N,)) , ð›‰;  show_trace = false)
        # TODO Investigate NaN for initial values of Jacobian
        #fit = curve_fit(model_homography!, jacobian_model_homography!, ð‡, reshape(reinterpret(Float64,vec(pts)),(4*N,)) , ð›‰;  inplace = true, show_trace = false, maxIter = 5)
        rms_errors[i] = sqrt(mean(sum(fit.resid.^2)))
    end
    rms_errors
end

function model_homography(ð‡,ð›‰)
    # 2 parameters per 2D point.
    N = Int(length(ð›‰)/ 2)
    indexâ‚ = SVector(1,2)
    indexâ‚‚ = SVector(3,4)
    reprojections = Matrix{Float64}(undef,4,N)
    i = 1
    for n = 1:N
        # Extract 2D point and convert to homogeneous coordinates
        ð¦ = hom(SVector{2,Float64}(ð›‰[i],ð›‰[i+1]))
        reprojections[indexâ‚,n] = homâ»Â¹(ð¦)
        reprojections[indexâ‚‚,n] = homâ»Â¹(ð‡ * ð¦)
        i = i + 2
    end
    reshape(reinterpret(Float64,vec(reprojections)),(4*N,))
end

function model_homography!(reprojections::Array{Float64,1},ð‡,ð›‰)
    # 2 parameters per 2D point.
    N = Int(length(ð›‰)/ 2)
    indexâ‚ = SVector(1,2)
    indexâ‚‚ = SVector(3,4)
    reprojections_view = reshape(reinterpret(Float64,reprojections),(4,N))
    i = 1
    for n = 1:N
        # Extract 2D point and convert to homogeneous coordinates
        ð¦ = hom(SVector{2,Float64}(ð›‰[i],ð›‰[i+1]))
        reprojections_view[indexâ‚,n] = homâ»Â¹(ð¦)
        reprojections_view[indexâ‚‚,n] = homâ»Â¹(ð‡ * ð¦)
        i = i + 2
    end
    reprojections
end

function jacobian_model_homography(ð‡,ð›‰)
    # 2 parameters per 2D point.
    N = Int(length(ð›‰) / 2)
    indexâ‚ = SVector(1,2)
    indexâ‚‚ = SVector(3,4)
    ð‰ = zeros(4*N,2*N)
    # Create a view of the jacobian matrix ð‰ and reshape it so that
    # it will be more convenient to index into the appropriate entries
    # whilst looping over all of the data points.
    ð‰v = reshape(reinterpret(Float64,ð‰), 4, N, 2*N)
    ð€ = SMatrix{2,3,Float64,6}(1,0,0,1,0,0)
    ðˆâ‚ƒ = SMatrix{3,3}(1.0I)
    i = 1
    for n = 1:N
        # Extract 3D point and convert to homogeneous coordinates.
        ð¦ = hom(SVector{2,Float64}(ð›‰[i], ð›‰[i+1]))

        # Derivative of residual in first and second image w.r.t 2D point in the
        # first image.
        âˆ‚ð«â‚_dð¦ = ð€ * ðˆâ‚ƒ
        âˆ‚ð«â‚‚_dð¦ = ð€ * âˆ‚homâ»Â¹(ð‡ * ð¦) * ð‡
    @.  ð‰v[indexâ‚,n,i:i+1] = âˆ‚ð«â‚_dð¦[:,indexâ‚]
    @.  ð‰v[indexâ‚‚,n,i:i+1] = âˆ‚ð«â‚‚_dð¦[:,indexâ‚]
        i = i + 2
    end
    ð‰
end

# TODO Investigate NaNs
function jacobian_model_homography!(ð‰::Array{Float64,2}, ð‡,ð›‰)
    Base.display(ð‰)
    pause
    # 2 parameters per 2D point.
    N = Int(length(ð›‰) / 2)
    indexâ‚ = SVector(1,2)
    indexâ‚‚ = SVector(3,4)
    # Create a view of the jacobian matrix ð‰ and reshape it so that
    # it will be more convenient to index into the appropriate entries
    # whilst looping over all of the data points.
    ð‰v = reshape(reinterpret(Float64,ð‰), 4, N, 2*N)
    ð€ = SMatrix{2,3,Float64,6}(1,0,0,1,0,0)
    ðˆâ‚ƒ = SMatrix{3,3}(1.0I)
    i = 1
    for n = 1:N
        # Extract 3D point and convert to homogeneous coordinates.
        ð¦ = hom(SVector{2,Float64}(ð›‰[i], ð›‰[i+1]))

        # Derivative of residual in first and second image w.r.t 2D point in the
        # first image.
        âˆ‚ð«â‚_dð¦ = ð€ * ðˆâ‚ƒ
        âˆ‚ð«â‚‚_dð¦ = ð€ * âˆ‚homâ»Â¹(ð‡ * ð¦) * ð‡
    @.  ð‰v[indexâ‚,n,i:i+1] = âˆ‚ð«â‚_dð¦[:,indexâ‚]
    @.  ð‰v[indexâ‚‚,n,i:i+1] = âˆ‚ð«â‚‚_dð¦[:,indexâ‚]
        i = i + 2
    end
    ð‰
end



#TODO Verify this...
function (::ParameterErrorAnalysis)(cameraâ‚::AbstractCamera, cameraâ‚‚::AbstractCamera, world::AbstractWorld, estimates::AbstractVector)
    analogue_imageâ‚ = get_image_type(cameraâ‚)
    analogue_imageâ‚‚ = get_image_type(cameraâ‚‚)

    # Project 3D points onto the cameras.
    aquire = AquireImageContext()
    â„³ = aquire(world, cameraâ‚)
    â„³â€² = aquire(world, cameraâ‚‚)

    errors = zeros(Float64, length(estimates))
    â„‹ = matrices(HomographyMatrices(cameraâ‚, cameraâ‚‚, get_planes(world)))
    for (i, estimate) in enumerate(estimates)
        ð‡áµ¢ = estimate
        ð¡áµ¢ = vec(ð‡áµ¢)
        ð›‰ = vec(â„‹[i])
        ðâ‚œ = UniformScaling(9) -  norm(ð›‰)^-2 * (ð›‰*ð›‰')
        errors[i] = norm(ðâ‚œ * ð¡áµ¢)^2
    end
    errors
end
